{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# The structure of GPflowOpt\n",
    "*Joachim van der Herten*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this document, the structure of the GPflowOpt library is explained, including some small examples. First the `Domain` and `Optimizer` interfaces are shortly illustrated, followed by a description of the `BayesianOptimizer`. At the end, a step-by-step walkthrough of the `BayesianOptimizer` is given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "The fundamtental design principle of GPflowOpt is solving optimization problems of the form\n",
    "$$\\underset{\\boldsymbol{x} \\in \\mathcal{X}}{\\operatorname{argmin}} f(\\boldsymbol{x}).$$ The *objective function* $f: \\mathcal{X} \\rightarrow \\mathbb{R}^p$ maps a candidate optimum to a score (or multiple). Here $\\mathcal{X}$ represents the input domain. This domain encloses all solutions to the optimization problem and can be entirely continuous (i.e., a $d$-dimensional hypercube) but may also consist of discrete and categorical parameters. \n",
    "\n",
    "In GPflowOpt, solving an optimization problem starts with defining the domain. This is achieved by combining parameters. This is how a simple square domain is defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table id='domain' width=100%><tr><td>Name</td><td>Type</td><td>Values</td></tr><tr><td>x1</td><td>Continuous</td><td>[-2.  2.]</td></tr><tr><td>x2</td><td>Continuous</td><td>[-1.  2.]</td></tr></table>"
      ],
      "text/plain": [
       "<GPflowOpt.domain.Domain at 0x7faa6c036f98>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from GPflowOpt.domain import ContinuousParameter\n",
    "domain = ContinuousParameter('x1', -2, 2) + ContinuousParameter('x2', -1, 2)\n",
    "domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this domain, we can now easily apply one of the included optimizers to optimize objective functions. GPflowOpt defines an intuitive `Optimizer` interface which can be used to specify the domain, the initial point(s), constraints (to be implemented) etc.\n",
    "Here is how a simple quadratic function is optimized using one of the available methods of SciPy's minimize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     fun: 0.0\n",
       "     jac: array([ 0.,  0.])\n",
       " message: 'Optimization terminated successfully.'\n",
       "    nfev: 3\n",
       "     nit: 2\n",
       "    njev: 2\n",
       "  status: 0\n",
       " success: True\n",
       "       x: array([[ 0.,  0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from GPflowOpt.optim import SciPyOptimizer\n",
    "\n",
    "def fx(X):\n",
    "    X = np.atleast_2d(X)\n",
    "    # Return objective & gradient\n",
    "    return np.sum(np.square(X), axis=1), 2*X\n",
    "\n",
    "\n",
    "optimizer = SciPyOptimizer(domain, method='SLSQP')\n",
    "optimizer.set_initial([-1,-1])\n",
    "optimizer.optimize(fx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective function should return both objectives, as well as gradients. Some methods are inherently gradient-free (like Monte-Carlo optimization) and automatically discard the second returned array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     fun: array([[ 0.00402339]])\n",
       " message: 'OK'\n",
       "    nfev: 201\n",
       " success: True\n",
       "       x: array([[-0.05970573,  0.0214152 ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from GPflowOpt.optim import MCOptimizer\n",
    "optimizer = MCOptimizer(domain, 200)\n",
    "optimizer.optimize(fx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In Bayesian Optimization, the typical assumption is that $f$ is expensive to evaluate and no gradients are available. Hence the typical approach is to sequentially select a limited set of decisions $\\boldsymbol{x}_0, \\boldsymbol{x}_1, ... \\boldsymbol{x}_{n-1}$ using a sampling policy. Hence each decision $\\boldsymbol{x}_i \\in \\mathcal{X}$ itself is the result of an optimization problem\n",
    "$$\\boldsymbol{x}_i = \\underset{\\boldsymbol{x}}{\\operatorname{argmax}} \\alpha_i(\\boldsymbol{x})$$\n",
    "\n",
    "Each iteration, a function $\\alpha_i$ which is cheap-to-evaluate acts as a surrogate for the expensive function. It is typically a mapping of the predictive distribution of a (Bayesian) model built on all decisions and their corresponding (noisy) evaluations. The mapping introduces an order in $\\mathcal{X}$ to obtain a certain goal. The typical goal within the context of Bayesian Optimization is the search for *optimality* or *feasibility* while keeping the amount of required evaluations ($n$) a small number. As we can have several functions $f$ representing objectives and constraints, Bayesian Optimization may invoke several models and mappings $\\alpha$. These mappings are typically referred to as *acquisition functions* (or *infill criteria*).\n",
    "\n",
    "In GPflowOpt this typical flow itself is implemented as follows:\n",
    "\n",
    "* Several acquisitions are available\n",
    "\n",
    "* The optimization flow is implemented in its own optimizer which complies with the `Optimizer` interface.\n",
    "\n",
    "Here the previous example is optimized using Bayesian Optimization instead, using the Expected Improvement acquisition function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javdrher/.virtualenvs/gpflowopt/lib/python3.5/site-packages/GPflow/transforms.py:129: RuntimeWarning: overflow encountered in exp\n",
      "  result = np.log(1. + np.exp(x)) + self._lower\n",
      "/home/javdrher/.virtualenvs/gpflowopt/lib/python3.5/site-packages/GPflow/transforms.py:140: RuntimeWarning: overflow encountered in exp\n",
      "  result = np.log(np.exp(y - self._lower) - np.ones(1, np_float_type))\n",
      "/home/javdrher/.virtualenvs/gpflowopt/lib/python3.5/site-packages/GPflow/transforms.py:140: RuntimeWarning: divide by zero encountered in log\n",
      "  result = np.log(np.exp(y - self._lower) - np.ones(1, np_float_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: inf or nan in gradient: replacing with zeros\n",
      "Warning: inf or nan in gradient: replacing with zeros\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "     fun: array([  4.19905705e-05])\n",
       " message: 'OK'\n",
       "    nfev: 10\n",
       " success: True\n",
       "       x: array([[  4.11478891e-09,  -6.48001316e-03]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from GPflowOpt.bo import BayesianOptimizer\n",
    "from GPflowOpt.design import FactorialDesign\n",
    "from GPflowOpt.acquisition import ExpectedImprovement\n",
    "import GPflow\n",
    "\n",
    "# The Bayesian Optimizer does not expect gradients to be returned\n",
    "def fx(X):\n",
    "    X = np.atleast_2d(X)\n",
    "    # Return objective & gradient\n",
    "    return np.sum(np.square(X), axis=1)[:,None]\n",
    "\n",
    "    \n",
    "X = FactorialDesign(2, domain).generate()\n",
    "Y = fx(X)\n",
    "\n",
    "# initializing quite a standard BO model, Gaussian Process Regression with\n",
    "# Matern52 ARD Kernel\n",
    "model = GPflow.gpr.GPR(X,Y,GPflow.kernels.Matern52(2, ARD=True))\n",
    "alpha = ExpectedImprovement(model)\n",
    "\n",
    "# Now we must specify an optimization algorithm to optimize the acquisition \n",
    "# function, every iteration. \n",
    "acqopt = SciPyOptimizer(domain, method='SLSQP')\n",
    "\n",
    "# Now create the Bayesian Optimizer\n",
    "optimizer = BayesianOptimizer(domain, alpha, optimizer=acqopt)\n",
    "optimizer.optimize(fx, n_iter=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This brief snippet code starts from a 2-level grid (square corner points) and uses a GP model to model the response surface of the objective function. An acquisition function is specified. The `BayesianOptimizer` follows the same interface as the other optimizers and is initialized with a domain, the acquisition function and an additional optimization method to optimize the acquisition function each iteration. Finally, it runs for 10 iterations to optimize fx.\n",
    "\n",
    "The code to evaluate the acquisition function on the model is written in TensorFlow, allowing gradient-based optimization without additional effort due to the automated differentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-by-step description of the BayesianOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
