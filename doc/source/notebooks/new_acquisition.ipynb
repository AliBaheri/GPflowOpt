{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining new acquisition functions\n",
    "*Joachim van der Herten*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPflowOpt implements supports some acquisition functions for common scenarios, such as EI and PoF. However, it is straightforward to implement your own strategy. For most strategies, it is sufficient to implement the `Acquisition` interface. In case a more sophisticated model is needed, this can easily be achieved with GPflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import copy\n",
    "import GPflow\n",
    "import GPflowOpt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(5)\n",
    "def camelback(X):\n",
    "    f = (4. - 2.1*X[:,0]**2 + 0.3* X[:,0]**4) * X[:,0]**2 + np.prod(X,axis=1) + 4 * (X[:,1]**2-1) * X[:,1]**2\n",
    "    return f[:,None] + rng.rand(X.shape[0], 1) * 1\n",
    "\n",
    "# Setup input domain\n",
    "domain = GPflowOpt.domain.ContinuousParameter('x1', -3, 3) + \\\n",
    "         GPflowOpt.domain.ContinuousParameter('x2', -2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example on how to implement a custom acquisition function, we illustrate the Augmented EI (Huang et al. 2006), a modification for Expected Improvement for optimization of noisy functions. It is defined as\n",
    "$$\n",
    "\\alpha_{\\text{aEI}}(\\mathbf x_{\\star}) = \\alpha_{\\text{EI}}(\\mathbf x_{\\star}) \\left( 1 - \\frac{\\sigma_n}{\\sqrt{\\text{Var}\\left[ f_{\\star}\\,|\\, \\mathbf x, \\mathbf y, \\mathbf x_{\\star} \\right] + \\sigma_n^2}}\\right)\n",
    "$$\n",
    "\n",
    "This definition can be interpreted as rescaling of the EI score, with respect to the noise variance. For $\\sigma^2_n=0$, the rescale term equals 1 and normal EI is recovered. For $\\sigma^2_n > 0$, small prediction variances are punished, which decreases concentration of sampling and enhances exploration.  \n",
    "\n",
    "To implement this acquisition function, we could override the `build_acquisition` method of `Acquisition`, however in this case its easier to override `ExpectedImprovement` and obtain the EI score through a call to build_acquisition of the parent class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AugmentedEI(GPflowOpt.acquisition.ExpectedImprovement):\n",
    "    def __init__(self, model):\n",
    "        super(AugmentedEI, self).__init__(model)\n",
    "\n",
    "    def build_acquisition(self, Xcand):\n",
    "        ei = super(AugmentedEI, self).build_acquisition(Xcand)\n",
    "        _, pvar = self.models[0].build_predict(Xcand)\n",
    "        return tf.multiply(ei, 1 - tf.sqrt(self.models[0].likelihood.variance)\n",
    "                           / (tf.sqrt(pvar + self.models[0].likelihood.variance ** 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This small experiment on the six hump camelback illustrates impact of the penalty term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "design = GPflowOpt.design.LatinHyperCube(9, domain)\n",
    "X = design.generate()\n",
    "Y = camelback(X)\n",
    "m = GPflow.gpr.GPR(X, Y, GPflow.kernels.Matern52(2, ARD=True, lengthscales=[10,10], variance=10000))\n",
    "m.likelihood.variance = 1\n",
    "m.likelihood.variance.fixed = True\n",
    "ei = GPflowOpt.acquisition.ExpectedImprovement(m)\n",
    "m = GPflow.gpr.GPR(X, Y, GPflow.kernels.Matern52(2, ARD=True, lengthscales=[10,10], variance=10000))\n",
    "m.likelihood.variance = 1\n",
    "m.likelihood.variance.fixed = True\n",
    "aei = AugmentedEI(m)\n",
    "\n",
    "opt = GPflowOpt.optim.StagedOptimizer([GPflowOpt.optim.MCOptimizer(domain, 200), \n",
    "                                       GPflowOpt.optim.SciPyOptimizer(domain)])\n",
    "\n",
    "bopt1 = GPflowOpt.BayesianOptimizer(domain, ei, optimizer=opt, scaling=False)\n",
    "with bopt1.silent():\n",
    "    bopt1.optimize(camelback, n_iter=50)\n",
    "\n",
    "bopt2 = GPflowOpt.BayesianOptimizer(domain, aei, optimizer=opt, scaling=False)\n",
    "with bopt2.silent():\n",
    "    bopt2.optimize(camelback, n_iter=50)\n",
    "\n",
    "f, axes = plt.subplots(1,2, figsize=(14,7))\n",
    "\n",
    "Xeval = GPflowOpt.design.FactorialDesign(101, domain).generate()\n",
    "Yeval = camelback(Xeval)\n",
    "titles = ['EI', 'AEI']\n",
    "shape = (101, 101)\n",
    "\n",
    "for ax, t, acq in zip(axes, titles, [ei, aei]):\n",
    "    pred = acq.models[0].predict_f(Xeval)[0]\n",
    "    ax.contourf(Xeval[:,0].reshape(shape), Xeval[:,1].reshape(shape), \n",
    "                pred.reshape(shape))\n",
    "    ax.set_xlabel('x1')\n",
    "    ax.set_ylabel('x2')\n",
    "    ax.set_title(t)\n",
    "    ax.scatter(acq.data[0][:,0], acq.data[0][:,1], c='w')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
